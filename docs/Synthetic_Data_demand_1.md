# SyntheticData

最近Epoch AI宣称大模型在2026年将会面临数据危机，我对此也比较关注。所以我觉得有必要写一篇小记，梳理一下模型训练对数据的需求，并论述一下大模型的训练数据究竟会在什么时候耗尽。

# 一、Scaling Law 估计需求
为了先对全局有一个较好的把握，我们先从经验公式开始。大模型的Scaling Law经验公式主要有两个，分别是**KM Scaling Law**(Open AI 2020年1月的工作，我们简称KM)和**ChinChilla Scaling Law**(DeepMind 2022年9月的工作，简称ChinChilla)。

**KM**树立了一个大模型性能(Loss的下降)将会随着模型参数量、计算量、数据量的不断增加而幂指数上升的信念，OpenAI指出Loss和这三者的关系大约是-0.05到-0.1次方的关系，而**ChinChilla**则主要指出了这三者联合关系的经验公式。

### 简单入门 : KM Law 
通过选用充分大参数模型进行训练的方法，OpenAI估计出了Loss和数据量之间大致呈以下关系：

L(D) = \left( \frac{D_c}{D} \right)^{\alpha_D}

其中 $\alpha_D = 0.095$ ， $D_c = 5.4 \times 10^{13}$ (tokens)，尽管1 token 通常大于1B，但大致可以理解为 $54TB$。

**粗糙的估计 :**  为了达到Loss=2，需要36GB的数据；Loss=1时，自然是需要五十TB左右的数据量，而Loss=0.5时就需要大约8万TB的数据量了。

### 联合关系 : ChinChilla Law & KM Law

**ChinChilla**推导的方式非常简单，尽管工作量堪称骇人，以下三条路都试了个遍：

- **控制计算量不变:**  不断尝试模型参数-数据 对应到Loss的最佳组合，从而得到在特定计算量下为达到最佳效果，计算量、参数量和数据量三者之间的关系；接下来逐步增大控制计算量的值，得到更多的三元组，从而进行外推。

- **控制参数量不变:**  不断尝试模型计算-数据 对应到Loss的最佳组合，得到三元组，继续外推。

- **拟合一个损失函数:**  这时直接利用大量实验点三元组数据，猜测出了在 参数-计算量关系图中的损失函数等高线，从而估计出了最佳组合并进行外推。

最终三种方法得到了一致的联合关系(DeepMind真有卡啊)：
$$L(N, D) = E + \frac{A}{N^{\alpha}} + \frac{B}{D^{\beta}}$$
再次说明了幂律关系。

**KM**也同样得到了一个有趣结论：

$L(N, D) = \left( \left( \frac{N_c}{N} \right)^{\frac{\alpha_N}{\alpha_D}} + \frac{D_c}{D} \right)^{\alpha_D}$
经验地：

$\alpha_N = 0.076 \quad \alpha_D = 0.103 \quad N_c = 6.4 \times 10^{13} \quad D_c = 1.8 \times 10^{13}$

作者基于假设：过拟合时
$\frac{L(N, D)}{L(N, \infty)} > 1.02$
得到了D和N之间的参数关系：
$D \geq 5 \times 10^{3} \times N^{0.74}$

好了，接下来我们就可以做一些更有趣的估计了，比如：
假设我们是OpenAI的GPT-5训练工程师，公司为我们准备了1000TB的数据量(**大致相当于全球所有书店的总和！**) 我们可以用它训练一个多大的模型，性能大约又会有多好：

**参数量**：我们不妨估计这相当于 $10^{15}$ 个token，为了不过拟合，模型参数最多只能有 $1.87 \times 10^{15}$，也就是187,000 Billion，大致相当于GPT-3.5的1000倍，这才是真正的大模型！

**Loss**：依据KM Law，我们可以估计到它的Loss大致只有0.74，这相当了不起：GPT-3.5大致在1.8，GPT-4按照估计则大致是1.3。

**计算开销**：

$$C=6ND$$
得到大致是 $1.122 \times 10^{31}$ 次浮点运算，真正的天文数字。一台H100的FP32/16吞吐速度大致是 $2 \times 10^{15}$ (2000万亿)次每秒，这样一台H100推理大概一亿八千万年就可以完成运算了。不过我们觉得还是两年之内训练出来比较好，这样就要大约九千万台H100(每台30万美元)，买卡的钱就要27万亿美金。

**低配版** 奥特曼看到我们的预算之后紧急喊停，并且指出按照当前实践，数据量和参数量的比例大致是100比1，而且还有继续升高的趋势，我们于是修改了方案，现在的**参数量**大致是1,000 Billion，和网上爆出的GPT-5的参数量估计差不多；**Loss**将为1.15，也是相当的进步了；单H100训练的时间为95万年，两年期要部署475,647台H100，卡费1500亿美金，大致相当于十个航母战斗群。

### 实践中的数据需求

注意到计算开销是

$$C=6ND$$

在大量大模型的训练实践中，主要的瓶颈现在落在训练所需的庞大计算量和模型推理的算力上；因此，大模型工程师现在越来越喜欢用充分数据量来训练一个参数量相对较小的模型。

**数据：模型参数比** Qwen平均17条token比1个参数；Llama-3为215:1；最新锐的开源模型phi-3-medium为343:1；浓缩版的phi-3-mini为869:1。我们可以看到模型越小、算力资源越缺乏，这个比例会越高；然而，Chinchilla的提出者DeepMind认为过百的比例还是过高，已经到了过度训练的地步了。

**小估计**：按1:20来算，70 Billion参数的模型对应的数据量大致为 $1.4 \times 10^{12}$ token，大致相当于百亿行语句/算式。

# 二、数据的真实需求

### **一般语料**：
直觉上说，大模型对数据的格式要求并不严格，打个粗糙的比方，大模型基于数据进行训练的过程就像小孩读书启蒙世界观和语言能力，名著、小说还是微博评论区对通识能力的涌现并没有特别大的影响；前期数据清洗时主要的工作一般只是去除乱码留下文本。

### **专用语料**：
然而，一旦要求模型具有专门能力，我们就必须为针对性训练准备数以十亿字计的相应文字材料，例如我们希望模型拥有精湛地Weierstrass定义求表达式极限的能力(90%@1Pass)，就需要准备上亿条高等数学领域的定义、算式和上千万条运用Weierstrass定义求解的例题；从这个角度来讲，专用语料不用等到2026年，在当下就面临着严重的短缺。

**小估计**：一个最典型的专用数据类型的例子就是代码，我们可以估计一下当前模型为了训得代码能力可以找到的数据量：据Wikipedia 2022记录，Github上的总仓库数量已经超过了1.9亿个，其中的开源仓库超过了2800万个；我的室友课程大作业1700行代码，我们假设每个repo都有这个数量级，那么开源的代码数量就将超过 $4.76 \times 10^{11}$ 行，或许有些小众语言还缺乏训练数据，但是作为整体，我不建议任何startup下场做代码数据合成。

# 三、结论
**量级** 基于上面的论述，我们可以粗糙地认为，想让大模型具备某种***语言习惯***，大致要为它准备数亿行语句或数百亿token，小于这个量级的合成数据可以被认为不具价值。

**价格** 另外估计如果训练方愿意为数据支付十分之一的总费用，这
批100亿token的数据价格大致就会在1000万到10亿量级不等。

**耗尽时间** 专用语料可以评价为早就耗尽了(从来就没够过)，等不到2026年；至于通用语料耗尽时间大致等同大气层空气耗尽时间。

**场景** 可以指出的一部分存在合成数据想象空间的领域，包括合成数学推导、合成硬件编程语言、合成特定操作系统指令(比如无人机控制指令)，其他的也欢迎大家补充 😺。

