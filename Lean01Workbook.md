# Lean4数据集的建设
[原链接](https://arxiv.org/html/2406.03847v2)

### 数据构建流程

在这一部分中，我们将详细描述如图2所示的整个流水线，逐步翻译和筛选正确样本，并演示最终的数据集构建过程。

#### 图2说明
图2展示了我们的流水线主要流程。从初始训练数据开始，我们微调翻译模型，并将其应用于自然语言问题集。翻译后的数据通过 Lean 4 编译、回译和自然语言推理 (NLI) 测试及人工诊断进行筛选。如果标注者认为样本达到了足够的准确性，则导出这些筛选后的样本。

#### 4.1 第一轮流水线
首先，我们从 MiniF2F（zheng2021minif2f）和 ProofNet（azerbayev2023proofnet）中收集 Lean 4 形式化陈述及其对应的自然语言问题。由于我们不在 MiniF2F 和 ProofNet 上测试自动形式化，因此使用这两个数据集的所有样本。

证明将使用“:= sorry”声明。所有样本对将从两个方向组织到训练数据中，以实现形式化语言和自然语言之间的双向翻译。我们还包括多任务 Lean 4 指令数据，包括证明定理、预测下一个策略，并使用自然语言解释 Lean 证明（如 pact 和 jiang2023multilingual）进行训练。

训练数据可以分为证明问题和有明确答案的问题。然而，Lean 4 只支持证明问题，所以我们通过添加证明目标重新措辞所有解决方案问题。具体来说，我们在原始自然问题后附加“证明它是{答案}。”，而形式化陈述中的证明目标则更改为证明解决方案应该是正确答案。

第一轮数据收集被送入我们的翻译模型，该模型从 InternLM-Math-Plus-20B（2023internlm）初始化，预训练于 Lean 相关数据集。该模型以学习率 $4 \times 10^{-5}$ 进行三轮微调，每个翻译方向使用两个不同但固定的提示。这些翻译数据作为进一步迭代的起点。微调使用32个A100 GPU，数小时内可完成。

在训练翻译模型后，我们希望改进模型在不同数学主题上形式化问题的能力。我们从数学竞赛论坛5收集数学问题作为主动学习数据集。该数据集包含中学到高中的数学问题，难度不一，涵盖奥林匹克水平。我们利用 Qwen-1.5-14B-Chat（qwen）从论坛帖子中提取问题、解决方案和正确答案，使用以下提示：

```
你是数据标注者。以下是数学学生之间的讨论，可能包含多个问题和多个解决方案。请以 JSON 格式提取它们。每个问题是一个元素，包含键包括 problem（字符串，不能遗漏任何假设，如非负数），answer（计算问题返回数字字符串，证明问题返回空字符串），和 tags（字符串列表）。标签应识别数学问题的类别。可能的标签包括：方程，不等式，数论，代数，概率，组合，三角函数等。
```

注意到某些问题不适合形式化，同时提取过程不稳定，可能导致不合适的问题。首先，我们只保留以下标签的问题：不等式、数论、三角函数、模运算、归纳法、函数方程、复数和多项式。其次，我们询问 Qwen 模型问题是否定义明确，使用以下提示：

```
请检查以下数学问题是否定义明确？请遵循以下规则：
1. 考虑问题中给出的每个条件，如果某个变量未在问题中定义，则问题不明确。
2. 如果问题包含多个目标或没有明确目标，则问题不明确。
3. 注意不等式可能省略变量为实数的声明，但它们是明确的，不要判断为不明确。
4. 请在最后一句话中回复 **well-defined** 或 **ill-defined**，确保不误判明确的问题。
```

我们筛除不明确的问题。手动检查几乎没有留下不明确的问题，尽管少数明确的问题被错误忽略。清理后，我们使用初始翻译模型将所有筛选后的问题翻译为形式化陈述。

明确定义的子集总计包含6652个不同标签，其中223个标签包含超过100个样本。这些标签涵盖从竞赛级别知识点到高中课程的大量问题。超过三分之二的样本标有代数相关标签，而几乎没有几何相关标签。还注意到某些标签错误，特别是“数论”标签经常被分配给不等式问题。根据这些发现，在后续分析中，我们将继续关注样本超过100个的标签，并在人工诊断过程中特别注意错误标签。

#### 4.2 数据诊断和迭代流程
**编译正确性测试**
为了确保我们的翻译流水线生成的形式化陈述的准确性，每个翻译的形式化定理都在 Lean 4 环境中进行正确性检查。最初，定理陈述独立验证，使用“by sorry”占位符过滤出不正确的陈述。然后检查完整的定理，包括证明。此步骤的主要瓶颈是 Lean 4 项目的编译成本。为了加速过程，我们建立了一个 Lean 4 读-评-打印循环（REPL），利用 Lean 4 的运行时元编程功能，允许在解释模式下验证 Lean 4 陈述。正确性测试程序可以以多进程方式执行，使用32核 CPU 一小时内完成。我们的测试环境基于 Lean v4.7.0，使用相同版本的 Mathlib4（可以通过指定标签 v4.7.0 克隆）。Minif2f 环境等同于 https://huggingface.co/datasets/internlm/Lean-Workbook/discussions/1 中的头文件。

**数据过滤**
首先，通过编译正确性测试处理所有问题的合成翻译。然而，通常看到正确编译的翻译实际上并未遵循原始问题。第二步过滤基于我们的模型的回译能力。在形式化陈述回译为自然问题后，我们可以使用通用领域的 LLM 利用其自然语言推理能力。在我们的流水线中，我们继续查询 Qwen-1.5-14B-Chat 判断原始问题是否与回译版本相同。如果未得到肯定答复，样本标记为需要人工修正。提示如下：

```
请检查以下两个数学问题是相同还是不同？请考虑两个问题中的每个陈述，如果任何陈述不同，它们就是不同的。请指出发现的任何差异。请在最后一句话中回复 **same** 或 **different**，并用粗体格式标出。
```

**诊断和人工标注**
数据诊断主要关注两种样本：未通过编译正确性测试的样本，以及通过测试但未通过 NLI 反馈的正确翻译样本。其他通过 NLI 测试的样本暂时认为是正确的。在前三轮迭代中，这两种样本都有相对明显的模式。因此，我们总结并相应地修改这些样本，使用三位熟悉 Lean 和竞赛级数学问题的专家6。

手动修改的样本被添加到训练数据中，新翻译模型被微调，用于下一轮生成和筛选合成样本进行人工诊断。这两个过程与前段描述相同。每次迭代将平均添加约30个人工标注样本到训练数据中，解决当前模型的弱点。

经过几轮后，难以总结模式。我们改变诊断模式，按标签随机抽取数学问题。通过手动检查样本，将正确或修改的样本添加到训练数据中，并记录通过 NLI 测试的样本的正确率。每轮迭代将获得更多通过 NLI 测试的样本，正确率也会提高。在第六轮迭代后，正确率几乎达到95%，我们在迭代过程中共添加了341个问题到训练集。
